import sys
import os
import subprocess
from pyannote.audio import Pipeline
from pydub import AudioSegment
import whisper
from datetime import timedelta
import torch

# ==== ì„¤ì • ====
INPUT_DIR = "vods/"
INPUT_FILENAME_BASE = "ë°”ì‚­í† ìŠ¤íŠ¸ì˜ ë¼ì´ë¸Œ ë°©ì†¡"
INPUT_EXT = "mp3"

INPUT_FILENAME = f"{INPUT_FILENAME_BASE}.{INPUT_EXT}"
INPUT_FILE = os.path.join(INPUT_DIR, INPUT_FILENAME)

TEMP_WAV = "audio.wav"
HUGGINGFACE_TOKEN = "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"
WHISPER_MODEL = "large"

# ==== ë””ë°”ì´ìŠ¤ ì„¤ì • ====
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âš™ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")

# ==== 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ ====
def extract_audio(input_path, output_wav):
    try:
        ext = os.path.splitext(input_path)[1].lower()
        print(f"ğŸ¬ ì…ë ¥ íŒŒì¼: {input_path}")
        if ext in [".mp4", ".mkv"]:
            print(f"ğŸ”„ {ext[1:].upper()}ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ (16000Hz, mono)...")
            completed = subprocess.run([
                "ffmpeg", "-y", "-i", input_path,
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", output_wav
            ], capture_output=True, text=True)
            if completed.returncode != 0:
                print(f"âŒ ffmpeg ì˜¤ë¥˜:\n{completed.stderr}")
                sys.exit(1)
            print(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ â†’ {output_wav}")
        else:
            print("ğŸ§ ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜ ì¤‘ (16000Hz, mono)...")
            audio = AudioSegment.from_file(input_path).set_frame_rate(16000).set_channels(1)
            audio.export(output_wav, format="wav")
            print(f"âœ… ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ â†’ {output_wav}")
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ/ë³€í™˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

# ==== 2. í™”ì ë¶„ë¦¬ ====
def diarize_audio(wav_path, token):
    try:
        print("ğŸ” í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì¤‘...")
        pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=token)
        pipeline.to(DEVICE)  # âœ… CUDA ì‚¬ìš© ëª…ì‹œ
        print("ğŸ‘¤ í™”ì ë¶„ë¦¬ ìˆ˜í–‰ ì¤‘...")
        diarization = pipeline(wav_path)
        print("âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ")
        return diarization
    except Exception as e:
        print(f"âŒ í™”ì ë¶„ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

# ==== 3. Whisper ìŒì„± ì¸ì‹ ====
def transcribe_segments(wav_path, diarization, model):
    try:
        print("ğŸ—£ Whisper ìŒì„± ì¸ì‹ ì‹œì‘...")
        audio = AudioSegment.from_wav(wav_path)
        results = []
        segments = list(diarization.itertracks(yield_label=True))
        total = len(segments)
        for i, (turn, _, speaker) in enumerate(segments, 1):
            percent = (i / total) * 100
            print(f"â–¶ [{i}/{total}] ({percent:.1f}%) í™”ì: {speaker} | ì‹œê°„: {turn.start:.1f}s ~ {turn.end:.1f}s")
            start_ms = int(turn.start * 1000)
            end_ms = int(turn.end * 1000)
            segment = audio[start_ms:end_ms]
            segment_path = f"temp_{speaker}_{start_ms}.wav"
            segment.export(segment_path, format="wav")

            print("ğŸ”ˆ Whisper ì²˜ë¦¬ ì¤‘...")
            whisper_result = model.transcribe(
                segment_path,
                language="ko",
                fp16=torch.cuda.is_available(),  # âœ… mixed precision if using CUDA
            )
            text = whisper_result.get("text", "").strip()
            os.remove(segment_path)

            print(f"ğŸ“ ì¸ì‹ ê²°ê³¼: {text[:50]}{'...' if len(text) > 50 else ''}")
            results.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker,
                "text": text
            })
        print("âœ… Whisper ì¸ì‹ ì „ì²´ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ Whisper ì¸ì‹ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

# ==== 4. ê²°ê³¼ ì¶œë ¥ ====
def format_time(seconds):
    return str(timedelta(seconds=int(seconds)))

def write_output(results, output_file="transcript.txt"):
    try:
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘ â†’ {output_file}")
        with open(output_file, "w", encoding="utf-8") as f:
            for r in results:
                f.write(f"[{format_time(r['start'])}] {r['speaker']}: {r['text']}\n")
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

# ==== ì‹¤í–‰ ====
def main():
    if not os.path.exists(INPUT_FILE):
        print("INPUT_FILE ê²½ë¡œ:", INPUT_FILE)
        print("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:", os.path.exists(INPUT_FILE))
        sys.exit(1)

    extract_audio(INPUT_FILE, TEMP_WAV)

    diarization = diarize_audio(TEMP_WAV, HUGGINGFACE_TOKEN)

    try:
        whisper_model = whisper.load_model(WHISPER_MODEL, device=str(DEVICE))  # âœ… CUDA ëª¨ë¸ ë¡œë”©
        print("ğŸ“¦ Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ Whisper ëª¨ë¸ ë¡œë”© ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

    results = transcribe_segments(TEMP_WAV, diarization, whisper_model)

    # ì„ì‹œ ì „ì²´ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
    if os.path.exists(TEMP_WAV):
        os.remove(TEMP_WAV)
        print(f"ğŸ§¹ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {TEMP_WAV}")
        
    # ì¶œë ¥ íŒŒì¼ ì´ë¦„ ìë™ ìƒì„± (mp3 â†’ txt)
    base_name = os.path.splitext(INPUT_FILENAME)[0]
    output_txt = os.path.join(INPUT_DIR, f"{base_name}.txt")

    write_output(results, output_txt)

if __name__ == "__main__":
    main()
